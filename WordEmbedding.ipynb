{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1M7umAcIKTaQlwDD2Q4Y44b9r3mnhQ_iU",
      "authorship_tag": "ABX9TyN/MfM6DsSlM9IGtS+bIflS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebastianrix/FullStack-DotNetAPI-ReactFrontend/blob/main/WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "hKfFYohmaShJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = Path(\"/\")\n",
        "HCA_PATH = DATA_DIR / \"HCA.txt\"\n",
        "VOCAB_PATH = DATA_DIR / \"HCA_vocabulary.txt\"\n",
        "\n",
        "HCA_PATH.exists(), VOCAB_PATH.exists()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV1nySbEaRsC",
        "outputId": "80577d60-189b-4cd4-ba3f-7a81f5fa8991"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOU4qhdyX13w",
        "outputId": "8d1030e3-02a2-409f-d4ea-53ac1ab89c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 12273\n",
            "Contains ENDPERIOD: True\n"
          ]
        }
      ],
      "source": [
        "# Load sorted vocabulary list (one word per line)\n",
        "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab_list = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Build lookup tables\n",
        "word_to_id = {w: i for i, w in enumerate(vocab_list)}\n",
        "id_to_word = {i: w for w, i in word_to_id.items()}\n",
        "\n",
        "print(\"Vocab size:\", len(vocab_list))\n",
        "print(\"Contains ENDPERIOD:\", \"ENDPERIOD\" in word_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HCA.txt is space/newline separated words, already preprocessed\n",
        "with open(HCA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_words = f.read().split()\n",
        "\n",
        "print(\"Total tokens:\", len(corpus_words))\n",
        "print(\"Sample:\", corpus_words[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcCxheRwac26",
        "outputId": "61ae6937-6a67-4143-b7dd-927db75f62b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 396672\n",
            "Sample: ['in', 'the', 'garden', 'all', 'the', 'apple', 'trees', 'were', 'in', 'blossom', 'ENDPERIOD', 'they', 'had', 'hastened', 'to', 'bring', 'forth', 'flowers', 'before', 'they', 'got', 'green', 'leaves', 'and', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: verify all words are in vocab (corpus should match the provided vocabulary)\n",
        "missing = {w for w in set(corpus_words) if w not in word_to_id}\n",
        "print(\"Out-of-vocab words:\", len(missing))\n",
        "# Expect 0; if not, you can decide on an UNK policy.\n",
        "\n",
        "# Convert entire corpus to integer ids\n",
        "import numpy as np\n",
        "corpus_ids = np.fromiter((word_to_id[w] for w in corpus_words), dtype=np.int32)\n",
        "\n",
        "print(\"corpus_ids shape:\", corpus_ids.shape)\n",
        "print(\"First 20 ids:\", corpus_ids[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Segvvxafalfv",
        "outputId": "a316ec5a-5277-4022-fd1a-8cdc7d32d4ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-vocab words: 0\n",
            "corpus_ids shape: (396672,)\n",
            "First 20 ids: [ 5531 10857  4474   250 10857   419 11191 11908  5531  1079  3510 10881\n",
            "  4913  5023 11045  1290  4287  4167   865 10881]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- paths ---\n",
        "VOCAB_PATH = \"/HCA_vocabulary.txt\"\n",
        "TEXT_PATH  = \"/HCA.txt\"\n",
        "\n",
        "# --- load vocab (fixed order) ---\n",
        "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = [w.strip() for w in f if w.strip()]\n",
        "word2id = {w:i for i,w in enumerate(vocab)}\n",
        "id2word = {i:w for w,i in word2id.items()}\n",
        "V = len(vocab)  # should be 12273 in your set  :contentReference[oaicite:9]{index=9}\n",
        "\n",
        "# --- load tokens ---\n",
        "with open(TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    # file is space/newline separated tokens incl. ENDPERIOD  :contentReference[oaicite:10]{index=10}\n",
        "    tokens = f.read().split()\n",
        "\n",
        "# --- map to ids ---\n",
        "ids = [word2id[w] for w in tokens if w in word2id]  # all should be in vocab\n",
        "\n",
        "# --- build sliding-window dataset ---\n",
        "import numpy as np, tensorflow as tf\n",
        "\n",
        "n = 10          # context length (tune)\n",
        "EMB_D = 128     # embedding dim\n",
        "BATCH = 256     # tune\n",
        "BUFFER = 10000\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(len(ids) - n):\n",
        "    X.append(ids[i:i+n])\n",
        "    Y.append(ids[i+n])\n",
        "X = np.array(X, dtype=np.int32)\n",
        "Y = np.array(Y, dtype=np.int32)\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "eS5EcWuzfhog"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "7fROV3-ct9Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "\n",
        "def build_mlp_nextword(V, n, EMB_D, hidden=256):\n",
        "    inputs = keras.Input(shape=(n,), dtype=\"int32\")\n",
        "    x = L.Embedding(V, EMB_D, input_length=n)(inputs)   # (batch, n, EMB_D)\n",
        "    x = L.Flatten()(x)                                  # (batch, n*EMB_D)\n",
        "    x = L.Dense(hidden, activation=\"relu\")(x)\n",
        "    x = L.Dropout(0.2)(x)\n",
        "    outputs = L.Dense(V, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "mlp = build_mlp_nextword(V, n, EMB_D)\n",
        "mlp.summary()\n",
        "# mlp.fit(ds, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "4baIRfXEstrD",
        "outputId": "1fc5b387-bc21-47dd-d1b4-01dbdc80a6c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m1,570,944\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12273\u001b[0m)          │     \u001b[38;5;34m3,154,161\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,570,944</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12273</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,154,161</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,053,041\u001b[0m (19.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,053,041</span> (19.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,053,041\u001b[0m (19.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,053,041</span> (19.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.fit(ds, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOuDKpz7uGK8",
        "outputId": "7f122c52-bd36-4def-b99b-102fb0f73044"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 245ms/step - loss: 6.2917 - sparse_categorical_accuracy: 0.1105\n",
            "Epoch 2/5\n",
            "\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 252ms/step - loss: 5.2540 - sparse_categorical_accuracy: 0.1616\n",
            "Epoch 3/5\n",
            "\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 240ms/step - loss: 4.9138 - sparse_categorical_accuracy: 0.1816\n",
            "Epoch 4/5\n",
            "\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 242ms/step - loss: 4.6244 - sparse_categorical_accuracy: 0.1970\n",
            "Epoch 5/5\n",
            "\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 243ms/step - loss: 4.3454 - sparse_categorical_accuracy: 0.2148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7974d91bb140>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Choose a PAD id. If you have '<PAD>' in your vocab, use that. Otherwise 0 is common.\n",
        "PAD_ID = word2id.get(\"<PAD>\", 0)\n",
        "UNK_ID = word2id.get(\"<UNK>\", PAD_ID)  # fallback for unknown words\n",
        "\n",
        "def ids_from_words(words):\n",
        "    return [word2id.get(w, UNK_ID) for w in words]\n",
        "\n",
        "def make_ctx_ids(seed_ids, n, pad_id=PAD_ID):\n",
        "    \"\"\"Left-pad/truncate to exactly length n.\"\"\"\n",
        "    if len(seed_ids) < n:\n",
        "        seed_ids = [pad_id] * (n - len(seed_ids)) + seed_ids\n",
        "    else:\n",
        "        seed_ids = seed_ids[-n:]\n",
        "    return np.array(seed_ids, dtype=np.int32)[None, :]  # shape (1, n)\n",
        "\n",
        "def top_k_sample(probs, k=10, temperature=1.0):\n",
        "    probs = np.asarray(probs, dtype=np.float64)\n",
        "    # avoid k > vocab\n",
        "    k = min(k, probs.shape[0])\n",
        "\n",
        "    if temperature != 1.0:\n",
        "        logits = np.log(probs + 1e-9) / temperature\n",
        "        probs = np.exp(logits)\n",
        "        probs = probs / probs.sum()\n",
        "\n",
        "    idxs = np.argpartition(probs, -k)[-k:]\n",
        "    sub = probs[idxs]\n",
        "    sub = sub / sub.sum()\n",
        "    return int(np.random.choice(idxs, p=sub))\n",
        "\n",
        "def generate(model, seed_words, steps=50, k=10, T=1.0):\n",
        "    ctx_ids = ids_from_words(seed_words)\n",
        "    out_words = seed_words[:]  # start output with the seed\n",
        "\n",
        "    for _ in range(steps):\n",
        "        x = make_ctx_ids(ctx_ids, n=n, pad_id=PAD_ID)  # shape (1, n)\n",
        "        p = model.predict(x, verbose=0)[0]            # shape (V,)\n",
        "\n",
        "        wid = top_k_sample(p, k=k, temperature=T)\n",
        "        out_words.append(id2word[wid])\n",
        "        ctx_ids.append(wid)  # grow context; make_ctx_ids will keep last n\n",
        "\n",
        "    return out_words\n",
        "\n",
        "# Example:\n",
        "print(\" \".join(generate(mlp, [\"in\", \"the\", \"garden\"], steps=30, k=10, T=1.0)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcpFXpHT_soF",
        "outputId": "9dbb9978-e602-424e-cc2e-ba3ed9557c50"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the garden the little girl a little man but a great bird had come on my own room and was gone with the old house in the snow ENDPERIOD the wind shone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---- Config pulled from your existing setup ----\n",
        "# Assumes you already have: mlp (model), n (context length), V (vocab size), word2id, id2word\n",
        "PAD_ID = word2id.get(\"<PAD>\", 0)\n",
        "UNK_ID = word2id.get(\"<UNK>\", PAD_ID)\n",
        "\n",
        "# ---- Token helpers ----\n",
        "def ids_from_words(words, word2id, unk_id=0):\n",
        "    return [word2id.get(w, unk_id) for w in words]\n",
        "\n",
        "def make_ctx_ids(ctx_ids, n, pad_id=0):\n",
        "    if len(ctx_ids) < n:\n",
        "        ctx_ids = [pad_id] * (n - len(ctx_ids)) + ctx_ids\n",
        "    else:\n",
        "        ctx_ids = ctx_ids[-n:]\n",
        "    return np.array(ctx_ids, dtype=np.int32)[None, :]  # (1, n)\n",
        "\n",
        "# ---- Sampling ----\n",
        "def top_k_sample(probs, k=10, temperature=1.0):\n",
        "    probs = np.asarray(probs, dtype=np.float64)\n",
        "    k = max(1, min(k, probs.shape[0]))  # clamp k\n",
        "    if temperature != 1.0:\n",
        "        logits = np.log(probs + 1e-9) / temperature\n",
        "        probs = np.exp(logits); probs /= probs.sum()\n",
        "    idxs = np.argpartition(probs, -k)[-k:]\n",
        "    sub = probs[idxs]\n",
        "    sub = sub / sub.sum()\n",
        "    return int(np.random.choice(idxs, p=sub))\n",
        "\n",
        "def greedy_sample(probs):\n",
        "    return int(np.argmax(probs))\n",
        "\n",
        "# ---- Special tokens & detokenizer ----\n",
        "STOP_TOKENS = {\"ENDPERIOD\"}  # stop early if you want\n",
        "PUNCT_MAP = {\n",
        "    \"ENDPERIOD\": \".\",\n",
        "    \"ENDCOMMA\": \",\",\n",
        "    \"ENDQUESTION\": \"?\",\n",
        "    \"ENDCOLON\": \":\",\n",
        "    \"ENDSEMICOLON\": \";\",\n",
        "    \"ENDQUOTE\": '\"',\n",
        "    \"QUOTE\": '\"',\n",
        "}\n",
        "\n",
        "def detokenize(words):\n",
        "    out = []\n",
        "    for w in words:\n",
        "        if w in PUNCT_MAP:\n",
        "            if out:\n",
        "                out[-1] = out[-1] + PUNCT_MAP[w]\n",
        "            else:\n",
        "                out.append(PUNCT_MAP[w])\n",
        "        else:\n",
        "            out.append(w)\n",
        "    text = \" \".join(out).strip()\n",
        "    for p in [\".\", \",\", \"?\", \":\", \";\"]:\n",
        "        text = text.replace(\" \" + p, p)\n",
        "    text = \" \".join(text.split())\n",
        "    if text:\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text and text[-1] not in \".!?\":\n",
        "        text += \".\"\n",
        "    return text\n",
        "\n",
        "# ---- Core generators (NO retrain needed) ----\n",
        "def generate_tokens(\n",
        "    model,\n",
        "    seed_words,\n",
        "    n,\n",
        "    word2id,\n",
        "    id2word,\n",
        "    steps=50,\n",
        "    strategy=\"topk\",    # \"topk\" or \"greedy\"\n",
        "    k=10,\n",
        "    T=1.0,\n",
        "    pad_id=0,\n",
        "    stop_on_special=True\n",
        "):\n",
        "    ctx_ids = ids_from_words(seed_words, word2id, unk_id=UNK_ID)\n",
        "    out_words = seed_words[:]\n",
        "    for _ in range(steps):\n",
        "        x = make_ctx_ids(ctx_ids, n=n, pad_id=pad_id)\n",
        "        p = model.predict(x, verbose=0)[0]  # shape (V,)\n",
        "        wid = top_k_sample(p, k=k, temperature=T) if strategy == \"topk\" else greedy_sample(p)\n",
        "        w = id2word[wid]\n",
        "        out_words.append(w)\n",
        "        ctx_ids.append(wid)\n",
        "        if stop_on_special and w in STOP_TOKENS:\n",
        "            break\n",
        "    return out_words\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    seed_words,\n",
        "    steps=50,\n",
        "    strategy=\"topk\",\n",
        "    k=10,\n",
        "    T=1.0,\n",
        "    stop_on_special=True\n",
        "):\n",
        "    words = generate_tokens(\n",
        "        model, seed_words, n=n, word2id=word2id, id2word=id2word,\n",
        "        steps=steps, strategy=strategy, k=k, T=T,\n",
        "        pad_id=PAD_ID, stop_on_special=stop_on_special\n",
        "    )\n",
        "    return detokenize(words)\n",
        "\n",
        "# ---- Batch testing helpers ----\n",
        "def try_many(\n",
        "    prompts,\n",
        "    runs=3,\n",
        "    steps=30,\n",
        "    strategy=\"topk\",\n",
        "    k=10,\n",
        "    T=1.0\n",
        "):\n",
        "    for seed in prompts:\n",
        "        print(f\"\\nSeed: {seed}\")\n",
        "        for r in range(1, runs+1):\n",
        "            txt = generate_text(mlp, seed, steps=steps, strategy=strategy, k=k, T=T)\n",
        "            print(f\"  {r:>2}: {txt}\")\n",
        "\n",
        "# ---- Quick sanity checks ----\n",
        "print(generate_text(mlp, [\"could\", \"it\", \"be\"], steps=30, strategy=\"topk\", k=10, T=1.0))\n",
        "print(generate_text(mlp, [\"in\", \"the\", \"garden\"], steps=30, strategy=\"greedy\"))\n",
        "\n",
        "# Try many variants fast:\n",
        "try_many(\n",
        "    prompts=[\n",
        "        [\"could\", \"it\", \"be\"],\n",
        "        [\"once\", \"upon\", \"a\", \"time\"],\n",
        "        [\"the\", \"king\"],\n",
        "    ],\n",
        "    runs=3,\n",
        "    steps=25,\n",
        "    strategy=\"topk\",\n",
        "    k=10,\n",
        "    T=0.9\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeZHENUFBl58",
        "outputId": "25b07744-2d85-45ee-f353-b6384a5007a2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could it be a little and in this time they had been in the wood and the old man had been born and the old man who had been to have the man.\n",
            "In the garden the the old man and the old woman was the most beautiful and the wind had been a great deal of the old man and the old woman was a.\n",
            "\n",
            "Seed: ['could', 'it', 'be']\n",
            "   1: Could it be a good little girl the old woman and then there are so beautiful that i have taken away.\n",
            "   2: Could it be the most beautiful.\n",
            "   3: Could it be the little in the air where his wife.\n",
            "\n",
            "Seed: ['once', 'upon', 'a', 'time']\n",
            "   1: Once upon a time the old man had been seen by a year and the little man had fallen into the world but she could not bear him.\n",
            "   2: Once upon a time the light and the old woman had come and the sun had taken up and he looked at the duckling so.\n",
            "   3: Once upon a time and the old man was the most beautiful.\n",
            "\n",
            "Seed: ['the', 'king']\n",
            "   1: The king the only a great deal of all and the wind might go about it.\n",
            "   2: The king the mother the the wind had not to be so.\n",
            "   3: The king the the little girl was so large and it was a very little.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(mlp, [\"the\", \"od\", \"man\"], steps=30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukse7d2XDGgl",
        "outputId": "48b62093-6011-4faa-9c60-f71ff9d971cd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The od man the old old man and the little boy that it was the most lovely old bird.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def top_k_sample(probs, k=10, temperature=1.0):\n",
        "    probs = np.asarray(probs, dtype=np.float64)\n",
        "    if temperature != 1.0:\n",
        "        # temperature scaling\n",
        "        logits = np.log(probs + 1e-9) / temperature\n",
        "        probs = np.exp(logits)\n",
        "        probs = probs / probs.sum()\n",
        "    idxs = np.argpartition(probs, -k)[-k:]\n",
        "    sub = probs[idxs]\n",
        "    sub = sub / sub.sum()\n",
        "    return int(np.random.choice(idxs, p=sub))\n",
        "\n",
        "def generate(model, seed_words, steps=50, k=10, T=1.0):\n",
        "    ctx = [word2id[w] for w in seed_words]\n",
        "    out = seed_words[:]\n",
        "    for _ in range(steps):\n",
        "        x = np.array(ctx[-n:], dtype=np.int32)[None, :]\n",
        "        p = model.predict(x, verbose=0)[0]\n",
        "        wid = top_k_sample(p, k=k, temperature=T)\n",
        "        out.append(id2word[wid])\n",
        "        ctx.append(wid)\n",
        "    return out\n",
        "\n",
        "#Example:\n",
        "print(\" \".join(generate(mlp, [\"in\", \"the\", \"garden\"], steps=30)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_h549p8XtZOZ",
        "outputId": "e6b1fc43-27e1-4742-8a73-de7b578f9e88"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 10), found shape=(1, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4206741921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#Example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"the\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"garden\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4206741921.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, seed_words, steps, k, T)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mwid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 10), found shape=(1, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_nextword(V, n, EMB_D, hidden=256):\n",
        "    inputs = keras.Input(shape=(n,), dtype=\"int32\")\n",
        "    x = L.Embedding(V, EMB_D, input_length=n)(inputs)\n",
        "    x = L.LSTM(hidden)(x)\n",
        "    x = L.Dropout(0.2)(x)\n",
        "    outputs = L.Dense(V, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"sparse_categorical_accuracy\"])\n",
        "    return model\n",
        "\n",
        "lstm = build_lstm_nextword(V, n, EMB_D)\n",
        "lstm.summary()\n",
        "# lstm.fit(ds, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "s3Gn-J79tg_-",
        "outputId": "3316358d-dab9-4a80-c204-2e7af8eafb16"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m1,570,944\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m394,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12273\u001b[0m)          │     \u001b[38;5;34m3,154,161\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,570,944</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12273</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,154,161</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,119,345\u001b[0m (19.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,119,345</span> (19.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,119,345\u001b[0m (19.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,119,345</span> (19.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as L, Model\n",
        "\n",
        "class EmbeddingRegressor(Model):\n",
        "    def __init__(self, V, D, n, hidden=256):\n",
        "        super().__init__()\n",
        "        self.emb = L.Embedding(V, D, input_length=n, name=\"word_emb\")\n",
        "        self.encoder = L.LSTM(hidden)\n",
        "        self.head = L.Dense(D, name=\"pred_vec\")  # predict embedding\n",
        "        # We’ll compute loss manually vs. true embedding vectors\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.emb(x)           # (batch, n, D)\n",
        "        x = self.encoder(x)       # (batch, H)\n",
        "        y = self.head(x)          # (batch, D)\n",
        "        return y\n",
        "\n",
        "reg = EmbeddingRegressor(V, EMB_D, n)\n",
        "\n",
        "# Build a tf.data pipeline that also yields the *embedding* target\n",
        "def ds_for_embedding(ids, n, batch):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(ids)-n):\n",
        "        X.append(ids[i:i+n])\n",
        "        Y.append(ids[i+n])\n",
        "    X = tf.constant(X, dtype=tf.int32)\n",
        "    Y = tf.constant(Y, dtype=tf.int32)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER).batch(batch)\n",
        "\n",
        "    # map Y -> embedding vector\n",
        "    def to_vec(batch_x, batch_y):\n",
        "        # gather rows from the embedding table\n",
        "        emb_table = reg.emb.weights[0]  # (V, D) after first build/trace\n",
        "        y_vec = tf.gather(emb_table, batch_y)\n",
        "        return batch_x, y_vec\n",
        "    # force build to create weights\n",
        "    _ = reg(tf.zeros((1, n), dtype=tf.int32))\n",
        "    return ds.map(to_vec).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_vec = ds_for_embedding(ids, n, BATCH)\n",
        "\n",
        "reg.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "# reg.fit(ds_vec, epochs=5)\n"
      ],
      "metadata": {
        "id": "Wq2-tYrotnS6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Precompute embedding table once\n",
        "E = reg.emb.get_weights()[0]    # (V, D)\n",
        "\n",
        "def nearest_ids(vec, topk=10):\n",
        "    # cosine similarity (fast and scale-invariant)\n",
        "    v = vec / (np.linalg.norm(vec, axis=-1, keepdims=True) + 1e-9)\n",
        "    E_norm = E / (np.linalg.norm(E, axis=-1, keepdims=True) + 1e-9)\n",
        "    sims = E_norm @ v.T   # (V, batch) or (V,) if v is (D,)\n",
        "    idxs = np.argpartition(-sims[:,0], topk)[:topk]\n",
        "    # sort topk\n",
        "    idxs = idxs[np.argsort(-sims[idxs, 0])]\n",
        "    return idxs\n",
        "\n",
        "def generate_by_vec(model, seed_words, steps=50, topk=10):\n",
        "    ctx = [word2id[w] for w in seed_words]\n",
        "    out = seed_words[:]\n",
        "    for _ in range(steps):\n",
        "        x = np.array(ctx[-n:], dtype=np.int32)[None, :]\n",
        "        y_vec = model.predict(x, verbose=0)          # (1, D)\n",
        "        cands = nearest_ids(y_vec.T, topk=topk)      # (topk,)\n",
        "        wid = int(np.random.choice(cands))\n",
        "        out.append(id2word[wid])\n",
        "        ctx.append(wid)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "qUizWfHPttt3"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}